# å¿«é€Ÿå¼€å§‹æŒ‡å— (Quick Start Guide)

> **é¡¹ç›®**: å››æ¨¡æ€ç„Šæ¥ç¼ºé™·æ£€æµ‹ - Causal-FiLMæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹  
> **æ›´æ–°æ—¶é—´**: 2025å¹´11æœˆ10æ—¥

---

## ğŸ“‹ ç›®å½•

1. [ç¯å¢ƒå‡†å¤‡](#1-ç¯å¢ƒå‡†å¤‡)
2. [æ¨¡å‹é€‰æ‹©](#2-æ¨¡å‹é€‰æ‹©)
3. [Causal-FiLMä½¿ç”¨æŒ‡å—](#3-causal-filmä½¿ç”¨æŒ‡å—)
4. [Late Fusion Baselineä½¿ç”¨æŒ‡å—](#4-late-fusion-baselineä½¿ç”¨æŒ‡å—)
5. [Causal-FiLM + Video AE èåˆæŒ‡å—](#5-causal-film--video-ae-èåˆæŒ‡å—)
6. [SupConä½¿ç”¨æŒ‡å—](#6-supconä½¿ç”¨æŒ‡å—)
7. [é¢„æœŸè¾“å‡º](#7-é¢„æœŸè¾“å‡º)
8. [æ•…éšœæ’æŸ¥](#8-æ•…éšœæ’æŸ¥)

---

## 1. ç¯å¢ƒå‡†å¤‡

### 1.1 ç³»ç»Ÿè¦æ±‚

- Python 3.8+
- CUDA 11.0+ (GPUæ¨è)
- 16GB+ RAM
- 50GB+ ç£ç›˜ç©ºé—´

### 1.2 å®‰è£…ä¾èµ–

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/yourusername/Weld-Anomaly-Detection.git
cd Weld-Anomaly-Detection

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n weld python=3.9
conda activate weld

# å®‰è£…PyTorch (æ ¹æ®ä½ çš„CUDAç‰ˆæœ¬)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# å®‰è£…å…¶ä»–ä¾èµ–
pip install -r requirements.txt

# å®‰è£…CLIP (ç”¨äºCausal-FiLM)
pip install git+https://github.com/openai/CLIP.git

# å®‰è£… Mamba (ç”¨äº SensorModulator)
# æ³¨æ„ï¼šMamba éœ€è¦ CUDA ç¯å¢ƒ
pip install causal-conv1d>=1.2.0
pip install mamba-ssm

```

### 1.3 æ•°æ®å‡†å¤‡

å°†æ•°æ®é›†æ”¾ç½®åœ¨ `Data/` ç›®å½•ä¸‹ï¼Œç»“æ„å¦‚ä¸‹ï¼š

```
Data/
â”œâ”€â”€ 1_good_weld_1_02-09-23_Fe410/
â”œâ”€â”€ 2_good_weld_2_02-09-23_Fe410/
â”œâ”€â”€ 4_porosity_w_excessive_penetration/
â””â”€â”€ ...
```

---

## 2. æ¨¡å‹é€‰æ‹©

æœ¬é¡¹ç›®æä¾›å¤šç§æ¨¡å‹æ¶æ„ï¼š

| æ¨¡å‹ | ç±»å‹ | è®­ç»ƒæ•°æ® | ä¼˜åŠ¿ | ä½¿ç”¨åœºæ™¯ |
|------|------|----------|------|----------|
| **Causal-FiLM** (V5) | æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ | ä»…æ­£å¸¸æ ·æœ¬ | æ— éœ€æ ‡æ³¨å¼‚å¸¸ï¼Œæ³›åŒ–æ€§å¼º | **æ¨è**ï¼šå¼‚å¸¸æ ·æœ¬ç¨€ç¼º |
| **Late Fusion Baseline** | æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ | ä»…æ­£å¸¸æ ·æœ¬ | è®ºæ–‡åŸå§‹æ–¹æ³•ï¼Œå…¬å¹³å¯¹æ¯”åŸºå‡† | åŸºçº¿å¯¹æ¯”å®éªŒ |
| **SupCon** (V4) | ç›‘ç£å¯¹æ¯”å­¦ä¹  | æ­£å¸¸+å¼‚å¸¸æ ·æœ¬ | åˆ†ç±»ç²¾åº¦é«˜ | å¼‚å¸¸æ ·æœ¬å……è¶³ |

---

## 3. Causal-FiLMä½¿ç”¨æŒ‡å—

### 3.1 æ¶æ„æ¦‚è¿°

Causal-FiLMæ˜¯**æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹**æ¨¡å‹ï¼Œé€šè¿‡é‡å»ºå­¦ä¹ æ£€æµ‹å¼‚å¸¸ï¼š

- **L0**: å†»ç»“çš„ç‰¹å¾æå–å™¨ (V-JEPA, DINOv2, AST)
- **L1**: FiLMä¼ æ„Ÿå™¨è°ƒåˆ¶ (gamma/beta conditioning)
- **L2**: å› æœåˆ†å±‚ç¼–ç å™¨ (Process + Result)
- **L3**: åæ³›åŒ–è§£ç å™¨ (Linear Attention)
- **L4**: é‡å»ºæŸå¤± + CLIPæ–‡æœ¬çº¦æŸ

**æ ¸å¿ƒæ€æƒ³**: åªå­¦ä¹ "æ­£å¸¸"çš„å› æœæ˜ å°„ `f: Process â†’ Result`ï¼Œå¼‚å¸¸ä¼šäº§ç”Ÿå¤§çš„é‡å»ºè¯¯å·®ã€‚

### 3.2 å¿«é€Ÿè®­ç»ƒ

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ
bash scripts/train_causal_film.sh

# æˆ–ç›´æ¥è¿è¡ŒPython
python src/train_causal_film.py

# ä»æœ€ä½³æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
python src/train_causal_film.py --resume best

# ä»æœ€æ–°æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
python src/train_causal_film.py --resume latest

# ä»æŒ‡å®šæ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
python src/train_causal_film.py --resume /path/to/checkpoint_epoch_50.pth
```

**è®­ç»ƒå‚æ•°** (åœ¨ `configs/train_config.py` ä¸­é…ç½®):

- `batch_size`: 32
- `num_epochs`: 100
- `learning_rate`: 2e-5
- `lambda_text`: 0.1 (CLIPæŸå¤±æƒé‡)
- `early_stopping_patience`: 8

### 3.3 è¯„ä¼°

```bash
# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
bash scripts/evaluate_causal_film.sh /root/autodl-tmp/outputs/checkpoints/best_model.pth

# æŸ¥çœ‹ç»“æœ
cat /root/autodl-tmp/outputs/eval_results.json
```

**è¾“å‡ºæŒ‡æ ‡**:
- `I-AUROC`: å›¾åƒçº§æ£€æµ‹AUROC (Image-level Detection)
- `P-AUPRO@0.3`: åƒç´ çº§åˆ†å‰²AUPROï¼ŒFPRâ‰¤30% (Pixel-level Segmentation)
- `P-AUPRO@0.1`: åƒç´ çº§åˆ†å‰²AUPROï¼ŒFPRâ‰¤10%
- `P-AUPRO@0.01`: åƒç´ çº§åˆ†å‰²AUPROï¼ŒFPRâ‰¤1%
- `precision`, `recall`, `f1`: åœ¨æœ€ä¼˜é˜ˆå€¼ä¸‹çš„åˆ†ç±»æŒ‡æ ‡

### 3.4 æ¨ç†æµç¨‹

```python
import torch
from src.models import create_causal_film_model

# åŠ è½½æ¨¡å‹
model_config = {...}  # è§configs/model_config.py
model = create_causal_film_model(model_config)
checkpoint = torch.load("best_model.pth")
model.load_state_dict(checkpoint["model_state_dict"])
model.eval()

# æ¨ç†
with torch.no_grad():
    output = model(batch)
    anomaly_score = model.compute_anomaly_score(
        output["Z_result"],
        output["Z_result_pred"]
    )
    # score > threshold â†’ anomaly
```

---

## 4. Late Fusion Baselineä½¿ç”¨æŒ‡å—

### 4.1 æ¶æ„æ¦‚è¿°

Late Fusionæ˜¯è®ºæ–‡çš„åŸå§‹åŸºçº¿æ–¹æ³•ï¼Œç”¨äºå…¬å¹³å¯¹æ¯”å®éªŒï¼š

- **éŸ³é¢‘è‡ªç¼–ç å™¨**: 1D CNNï¼Œè¾“å…¥STFTé¢‘è°±å›¾
  - æ¶æ„ï¼šBatchNorm -> Conv -> 3Ã—Conv -> Bottleneck
  - å‚æ•°é‡ï¼š31.67M
  - è®­ç»ƒï¼š50 epochs, One-Cycle LR
  
- **è§†é¢‘è‡ªç¼–ç å™¨**: ä¸¤é˜¶æ®µæ¨¡å‹
  - Stage 1: å†»ç»“çš„SlowFastç‰¹å¾æå–
  - Stage 2: å…¨è¿æ¥è‡ªç¼–ç å™¨
  - è®­ç»ƒï¼šæœ€å¤§1000 epochsï¼Œæ—©åœ
  
- **åæœŸèåˆ**: 
  - æ ‡å‡†åŒ–ååŠ æƒç»„åˆ
  - æƒé‡åœ¨éªŒè¯é›†ä¸Šä¼˜åŒ– (w_audio=0.37, w_video=0.63)

### 4.2 è®­ç»ƒ

```bash
# è®­ç»ƒä¸¤ä¸ªè‡ªç¼–ç å™¨
bash baselines/Late_Fusion/train.sh --modality both

# ä»…è®­ç»ƒéŸ³é¢‘æ¨¡å‹
bash baselines/Late_Fusion/train.sh --modality audio

# ä»…è®­ç»ƒè§†é¢‘æ¨¡å‹
bash baselines/Late_Fusion/train.sh --modality video

# ä½¿ç”¨dummyæ•°æ®æµ‹è¯•
bash baselines/Late_Fusion/train.sh --modality both --dummy
```

### 4.3 è¯„ä¼°

```bash
# è¯„ä¼°å¹¶èåˆï¼ˆè‡ªåŠ¨ä¼˜åŒ–æƒé‡ï¼‰
bash baselines/Late_Fusion/evaluate.sh

# ä½¿ç”¨dummyæ•°æ®æµ‹è¯•
bash baselines/Late_Fusion/evaluate.sh --dummy
```

**è¾“å‡º**:
- éŸ³é¢‘æ¨¡å‹ Test AUC
- è§†é¢‘æ¨¡å‹ Test AUC
- èåˆæ¨¡å‹ Test AUC
- æ¯ç§ç¼ºé™·ç±»å‹çš„AUC
- ROCæ›²çº¿å¯¹æ¯”å›¾

### 4.4 é¢„æœŸç»“æœ

æ ¹æ®è®ºæ–‡æŠ¥å‘Šï¼š
- éŸ³é¢‘ AUC: ~0.8460
- è§†é¢‘ AUC: ~0.8977
- èåˆ AUC: ~0.9178

---

## 5. Causal-FiLM + Video AE èåˆæŒ‡å—ï¼ˆåºŸæ¡ˆï¼‰

ä¸ºäº†è¿›ä¸€æ­¥æå‡SOTAæ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†**èåˆç­–ç•¥**ï¼Œç»“åˆCausal-FiLMæ¨¡å‹ä¸ä¸“ç”¨çš„Video Autoencoderã€‚

### 5.1 è®­ç»ƒ Video Autoencoder

Video Autoencoder ä¸“é—¨ç”¨äºæ•æ‰è§†é¢‘/å›¾åƒä¸­çš„å¤–è§‚å¼‚å¸¸ï¼ˆå¦‚ Convexityï¼‰ã€‚

```bash
# è®­ç»ƒ Video Autoencoder (ä»…ç”¨æ­£å¸¸æ ·æœ¬)
bash scripts/train_video_ae.sh
```

### 5.2 è¯„ä¼°èåˆæ¨¡å‹

èåˆæ¨¡å‹ç»“åˆäº† Causal-FiLM å’Œ Video Autoencoder çš„åˆ†æ•°ã€‚

```bash
# è¯„ä¼°èåˆæ¨¡å‹ (éœ€å·²æœ‰ Causal-FiLM æƒé‡ checkpoints/best_model.pth)
bash scripts/evaluate_fusion.sh
```

---

## 6. SupConä½¿ç”¨æŒ‡å—

### 6.1 è®­ç»ƒ

```bash
# ä½¿ç”¨SupConè®­ç»ƒ
bash scripts/train.sh
```

### 6.2 è¯„ä¼°

```bash
# k-NNè¯„ä¼°
bash scripts/evaluate.sh
```

---

## 7. é¢„æœŸè¾“å‡º

### 7.1 Causal-FiLMè®­ç»ƒè¾“å‡º

```
======================================================================
INITIALIZING CAUSAL-FILM MODEL
======================================================================
  Total parameters: 45,234,567
  Trainable parameters: 2,345,678
  Output dimension: 128
  Device: cuda

======================================================================
STARTING TRAINING
======================================================================

Epoch 1/100
----------------------------------------------------------------------
  Epoch 1 [10/50] Loss: 0.3456
  Epoch 1 [20/50] Loss: 0.2987
  ...
  Train Loss: 0.2543 (Recon: 0.2134, CLIP: 0.0409)
  Val Loss: 0.2876 (Recon: 0.2456, CLIP: 0.0420)
  Mean Anomaly Score: 0.1234
  âœ… New best model! Val Loss: 0.2876

Epoch 2/100
----------------------------------------------------------------------
  ...
```

### 5.2 è¯„ä¼°è¾“å‡º

```
======================================================================
EVALUATING ON TEST SPLIT
======================================================================

Extracting anomaly scores...
  Processed 50/50 batches
  Total samples: 1600
  Normal samples: 800
  Anomaly samples: 800

Computing metrics...
  I-AUROC (Image-level Detection): 0.9235
  AP: 0.9104
  Optimal Threshold: 0.3456
  Precision: 0.8765
  Recall: 0.8654
  F1: 0.8709

  Computing P-AUPRO (Pixel-level Segmentation)...
    P-AUPRO@0.3: 0.9123
    P-AUPRO@0.1: 0.8876
    P-AUPRO@0.05: 0.8234
    P-AUPRO@0.01: 0.7654
```

---

## 8. æ•…éšœæ’æŸ¥

### 6.1 CLIPå¯¼å…¥é”™è¯¯

```bash
# é”™è¯¯: No module named 'clip'
pip install git+https://github.com/openai/CLIP.git
```

### 6.2 CUDAå†…å­˜ä¸è¶³

```python
# åœ¨train_config.pyä¸­å‡å°batch_size
"batch_size": 16,  # ä»32å‡åˆ°16
```

### 6.3 æ‰¾ä¸åˆ°æ­£å¸¸æ ·æœ¬

ç¡®ä¿æ•°æ®é›†ä¸­æœ‰æ ‡ç­¾åŒ…å«"good"æˆ–"normal"çš„æ ·æœ¬ï¼Œæˆ–åœ¨`train_causal_film.py`ä¸­è°ƒæ•´è¿‡æ»¤é€»è¾‘ã€‚

---

## é™„å½•: åŸSupConæµ‹è¯•è¾“å‡º

```
======================================================================
QUADMODAL MODEL TEST SUITE
======================================================================

======================================================================
Testing QuadModalSOTAModel (Dummy Encoders)
======================================================================

Model Configuration:
  Total parameters: 14,045,824
  Trainable parameters: 14,045,824
  Output dimension: 512

Input shapes:
  video: (4, 32, 3, 224, 224)
  post_weld_images: (4, 5, 3, 224, 224)
  audio: (4, 1, 128, 256)
  sensor: (4, 256, 6)

Forward pass (without attention)...
  Output shape: (4, 512)
  Output range: [-0.1971, 0.1968]
  âœ… Forward pass successful

Forward pass (with attention)...
  Output shape: (4, 512)
  Attention keys: ['video', 'image', 'audio', 'sensor']
  âœ… Attention weights returned

Testing gradient flow...
  âœ… All gradients valid

âœ… QuadModalSOTAModel (Dummy) test passed!


======================================================================
Testing create_quadmodal_model Factory
======================================================================

Model created via factory:
  Output dimension: 512
  Parameters: 14,045,824
  Output shape: (2, 512)

âœ… Factory function test passed!


======================================================================
Testing Encoder Freezing
======================================================================

Initial trainable parameters: 14,045,824
After freezing encoders: 3,409,408
After unfreezing encoders: 14,045,824

âœ… Encoder freezing test passed!


======================================================================
Testing Different Batch Sizes
======================================================================
  Batch size  1: (1, 512) âœ…
  Batch size  2: (2, 512) âœ…
  Batch size  4: (4, 512) âœ…
  Batch size  8: (8, 512) âœ…

âœ… All batch sizes passed!


======================================================================
Testing with Real DataLoader
======================================================================

DataLoader batch shapes:
  video: (2, 32, 3, 64, 64)
  post_weld_images: (2, 5, 3, 224, 224)
  audio: (2, 1, 64, 256)
  sensor: (2, 256, 6)
  labels: (2,)

Model output shape: (2, 512)

âœ… DataLoader integration test passed!


======================================================================
âœ… ALL QUADMODAL MODEL TESTS PASSED!
======================================================================
```

# æ•°æ®å¤„ç†
pip install pandas>=2.0.0
pip install scikit-learn>=1.3.0
pip install librosa>=0.10.0
pip install opencv-python

# å®éªŒè¾…åŠ©å·¥å…·
pip install einops
pip install tqdm
pip install numpy
```

**å¯é€‰ (ç”¨äºå®éªŒè·Ÿè¸ª):**
```bash
pip install wandb  # Weights & Biases
pip install tensorboard  # TensorBoard
```

### 1.4 ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ (å¯é€‰)

**å¦‚æœæœ‰ç½‘ç»œè¿æ¥çš„æœºå™¨:**

```bash
# å®‰è£… git-lfs
git lfs install

# åˆ›å»ºæ¨¡å‹æ–‡ä»¶å¤¹
mkdir -p models
cd models

# ä¸‹è½½ V-JEPA (è§†é¢‘ç¼–ç å™¨)
git clone https://hf-mirror.com/facebook/vjepa2-vitl-fpc64-256

# ä¸‹è½½ DINOv2 (å›¾ç‰‡ç¼–ç å™¨)
git clone https://hf-mirror.com/facebook/dinov2-base

# ä¸‹è½½ AST (éŸ³é¢‘ç¼–ç å™¨)
git clone https://hf-mirror.com/MIT/ast-finetuned-audioset-14-14-0.443

cd ..
```

**æ³¨æ„**: å¦‚æœæ²¡æœ‰é¢„è®­ç»ƒæ¨¡å‹,ä»£ç ä¼šè‡ªåŠ¨ä½¿ç”¨ Dummy ç‰ˆæœ¬è¿›è¡Œæµ‹è¯•ã€‚

---

## 2. ç¯å¢ƒæ£€æŸ¥

### 2.1 æ£€æŸ¥ PyTorch å®‰è£…

```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA version: {torch.version.cuda if torch.cuda.is_available() else \"N/A\"}')"
```

**é¢„æœŸè¾“å‡º:**
```
PyTorch: 2.1.0+cu118
CUDA available: True
CUDA version: 11.8
```

### 2.2 æ£€æŸ¥å…³é”®ä¾èµ–

```bash
python -c "import transformers; import timm; import pandas; import librosa; print('âœ… All dependencies installed')"
```

**é¢„æœŸè¾“å‡º:**
```
âœ… All dependencies installed
```

### 2.3 æ£€æŸ¥é¡¹ç›®ç»“æ„

```bash
python -c "import sys; sys.path.insert(0, '.'); from src.models import QuadModalSOTAModel; print('âœ… Project imports working')"
```

**é¢„æœŸè¾“å‡º:**
```
âœ… Project imports working
```

---

## 3. æ¨¡å—æµ‹è¯•

æŒ‰ç…§ä»¥ä¸‹é¡ºåºé€ä¸ªæµ‹è¯•å„ä¸ªæ¨¡å—,ç¡®ä¿æ¯ä¸ªç»„ä»¶æ­£å¸¸å·¥ä½œã€‚

### 3.1 æµ‹è¯•æ•°æ®åŠ è½½ (Step 1)

**è¿è¡Œæµ‹è¯•:**
```bash
bash scripts/test_dataset.sh
```

**é¢„æœŸè¾“å‡º:**
```
======================================================================
Testing WeldingDataset (Dummy Mode)
======================================================================

Configuration:
  Data root: Data
  Video length: 32
  Audio sample rate: 16000
  Audio duration: 2.0
  Sensor length: 256
  Image size: 224
  Number of angles: 5
  Dummy mode: True

Dataset created successfully
  Total samples: 100
  Sample keys: dict_keys(['video', 'post_weld_images', 'audio', 'sensor', 'label', 'sample_id'])

Single sample shapes:
  âœ… Video: (32, 3, 224, 224)
  âœ… Post-weld images: (5, 3, 224, 224)
  âœ… Audio: (1, 128, 256)
  âœ… Sensor: (256, 6)
  âœ… Label: scalar

Batch loading test:
  Batch size: 4
  âœ… Video batch: (4, 32, 3, 224, 224)
  âœ… Image batch: (4, 5, 3, 224, 224)
  âœ… Audio batch: (4, 1, 128, 256)
  âœ… Sensor batch: (4, 256, 6)
  âœ… Label batch: (4,)

âœ… All dataset tests passed!
```

**ç»“æœä¿å­˜**: æ—  (çº¯æµ‹è¯•,ä¸ä¿å­˜)

---

### 3.2 æµ‹è¯•ç¼–ç å™¨ (Step 2)

**è¿è¡Œæµ‹è¯•:**
```bash
bash scripts/test_encoders.sh
```

**é¢„æœŸè¾“å‡º:**
```
======================================================================
Testing VideoEncoder (Dummy Mode)
======================================================================
  Input: (2, 32, 3, 224, 224)
  Output: (2, 8, 1024)
  Parameters: 1,234,567
  âœ… VideoEncoder test passed

======================================================================
Testing ImageEncoder (Dummy Mode)
======================================================================
  Input: (2, 5, 3, 224, 224)
  Output: (2, 5, 768)
  Parameters: 987,654
  âœ… ImageEncoder test passed

======================================================================
Testing AudioEncoder (Dummy Mode)
======================================================================
  Input: (2, 1, 128, 256)
  Output: (2, 12, 768)
  Parameters: 876,543
  âœ… AudioEncoder test passed

======================================================================
Testing SensorEncoder
======================================================================
  Input: (2, 256, 6)
  Output: (2, 256, 256)
  Parameters: 654,321
  âœ… SensorEncoder test passed

======================================================================
Testing Gradients
======================================================================
  âœ… All gradients valid (no NaN/Inf)

âœ… ALL ENCODER TESTS PASSED!
```

### å®é™…è¾“å‡ºï¼ˆç¤ºä¾‹ï¼‰

ä¸‹é¢ä¸ºåœ¨å¼€å‘å®¹å™¨ä¸­ä¸€æ¬¡çœŸå®è¿è¡Œ `bash scripts/test_encoders.sh` çš„ç®€æ´ç¤ºä¾‹è¾“å‡ºï¼ˆä»…æ‘˜è¦ï¼‰ï¼š

```
Testing VideoEncoder
  DummyVideoEncoder -> Output: (2, 64, 1024) âœ…
  VideoEncoder (pretrained) -> Output: (2, 3136, 1024) âœ…

Testing AudioEncoder
  DummyAudioEncoder -> Output: (2, 32, 768) âœ…
  AudioEncoder (pretrained) -> Output: (2, 659, 768) âœ…

Testing ImageEncoder
  DummyImageEncoder -> Output: (2, 1, 768) âœ…
  ImageEncoder (pretrained) -> Output: (2, 257, 768) âœ…

Testing SensorEncoder
  SensorEncoder -> Output: (2, 256, 256) âœ…
```

æ­¤è¾“å‡ºè¡¨æ˜ï¼šåœ¨è¯¥ç¯å¢ƒä¸‹ï¼ˆæœ‰æœ¬åœ°æ¨¡å‹æˆ–å·²é€‚é…è¾“å…¥ï¼‰çœŸå®é¢„è®­ç»ƒç¼–ç å™¨å¯æˆåŠŸå‰å‘å¹¶è¿”å›ç‰¹å¾ç»´åº¦ï¼›è‹¥ä½ çš„ç¯å¢ƒæ²¡æœ‰æœ¬åœ°æ¨¡å‹ï¼Œå°†åªä¼šçœ‹åˆ° Dummy ç¼–ç å™¨çš„æµ‹è¯•é€šè¿‡ã€‚

**ç»“æœä¿å­˜**: æ—  (çº¯æµ‹è¯•,ä¸ä¿å­˜)

---

### 3.3 æµ‹è¯•èåˆæ¨¡å— (Step 3)

**è¿è¡Œæµ‹è¯•:**
```bash
bash scripts/test_fusion.sh
```

**é¢„æœŸè¾“å‡º:**
```
======================================================================
Testing CrossAttentionFusionModule
======================================================================

Configuration:
  Batch size: 4
  Video: seq_len=8, dim=1024
  Image: seq_len=5, dim=768
  Audio: seq_len=12, dim=768
  Sensor: seq_len=256, dim=256
  Hidden dim: 512

Model parameters:
  Total: 4,234,240
  Trainable: 4,234,240

Forward pass (without attention)...
  Output shape: (4, 512)
  Output range: [-2.3456, 3.1234]
  âœ… Output shape correct and values valid

Forward pass (with attention)...
  Output shape: (4, 512)
  Attention weights returned for: ['video', 'image', 'audio', 'sensor']
    video: (4, 4, 8)
    image: (4, 4, 5)
    audio: (4, 4, 12)
    sensor: (4, 4, 256)
  âœ… Attention weights shape correct

Testing gradient flow...
  âœ… All gradients valid

âœ… CrossAttentionFusionModule test passed!

======================================================================
Testing DummyCrossAttentionFusion (Lightweight)
======================================================================
  Model parameters: 1,573,376
  Output shape: (4, 512)
  âœ… DummyCrossAttentionFusion test passed!

======================================================================
Testing Fusion with Different Batch Sizes
======================================================================
  Batch size  1: (1, 512) âœ…
  Batch size  2: (2, 512) âœ…
  Batch size  8: (8, 512) âœ…
  Batch size 16: (16, 512) âœ…

âœ… All batch sizes passed!

======================================================================
âœ… ALL FUSION TESTS PASSED!
======================================================================
```

**ç»“æœä¿å­˜**: æ—  (çº¯æµ‹è¯•,ä¸ä¿å­˜)

---

### 3.4 æµ‹è¯•å®Œæ•´æ¨¡å‹ (Step 4)

**è¿è¡Œæµ‹è¯•:**
```bash
bash scripts/test_model.sh
```

**é¢„æœŸè¾“å‡º:**
```
======================================================================
Testing QuadModalSOTAModel (Dummy Encoders)
======================================================================

Model Configuration:
  Total parameters: 8,567,890
  Trainable parameters: 8,567,890
  Output dimension: 512

Input shapes:
  video: (4, 32, 3, 224, 224)
  post_weld_images: (4, 5, 3, 224, 224)
  audio: (4, 1, 128, 256)
  sensor: (4, 256, 6)

Forward pass (without attention)...
  Output shape: (4, 512)
  Output range: [-1.2345, 2.3456]
  âœ… Forward pass successful

Forward pass (with attention)...
  Output shape: (4, 512)
  Attention keys: ['video', 'image', 'audio', 'sensor']
  âœ… Attention weights returned

Testing gradient flow...
  âœ… All gradients valid

âœ… QuadModalSOTAModel (Dummy) test passed!

======================================================================
Testing create_quadmodal_model Factory
======================================================================
  Model created via factory:
  Output dimension: 512
  Parameters: 8,567,890
  Output shape: (2, 512)

âœ… Factory function test passed!

======================================================================
Testing Encoder Freezing
======================================================================
  Initial trainable parameters: 8,567,890
  After freezing encoders: 1,234,567
  After unfreezing encoders: 8,567,890

âœ… Encoder freezing test passed!

======================================================================
Testing Different Batch Sizes
======================================================================
  Batch size  1: (1, 512) âœ…
  Batch size  2: (2, 512) âœ…
  Batch size  4: (4, 512) âœ…
  Batch size  8: (8, 512) âœ…

âœ… All batch sizes passed!

======================================================================
Testing with Real DataLoader
======================================================================
  DataLoader batch shapes:
    video: (2, 32, 3, 224, 224)
    post_weld_images: (2, 5, 3, 224, 224)
    audio: (2, 1, 128, 256)
    sensor: (2, 256, 6)
    labels: (2,)
  
  Model output shape: (2, 512)

âœ… DataLoader integration test passed!

======================================================================
âœ… ALL QUADMODAL MODEL TESTS PASSED!
======================================================================
```

**ç»“æœä¿å­˜**: æ—  (çº¯æµ‹è¯•,ä¸ä¿å­˜)

---

### 3.5 æµ‹è¯•æŸå¤±å‡½æ•° (Step 5)

**è¿è¡Œæµ‹è¯•:**
```bash
bash scripts/test_losses.sh
```

**é¢„æœŸè¾“å‡º:**
```
======================================================================
LOSS FUNCTIONS TEST
======================================================================

Testing SupConLoss...
  Batch size: 8
  Feature dim: 512
  Num classes: 6
  Loss: 2.3456
  âœ… Backward pass successful
  Loss (all same class): 0.0001
  Loss (all different): 4.5678
  âœ… SupConLoss test passed!

Testing CombinedLoss...
  SupCon only:
    Total loss: 2.3456
    SupCon loss: 2.3456

  SupCon + CE:
    Total loss: 3.1234
    SupCon loss: 2.3456
    CE loss: 1.5558

  âœ… CombinedLoss test passed!

======================================================================
âœ… ALL LOSS TESTS PASSED!
======================================================================
```

**ç»“æœä¿å­˜**: æ—  (çº¯æµ‹è¯•,ä¸ä¿å­˜)

---

## 4. å®Œæ•´æµ‹è¯•

è¿è¡Œæ‰€æœ‰æµ‹è¯•ç¡®ä¿æ•´ä¸ªæµç¨‹æ­£å¸¸:

```bash
# é€ä¸ªè¿è¡Œæ‰€æœ‰æµ‹è¯•
bash scripts/test_dataset.sh
bash scripts/test_encoders.sh
bash scripts/test_fusion.sh
# bash scripts/test_model.sh
```
---

## 5. è®­ç»ƒæ¨¡å‹

### 5.0 é‡è¦ï¼šæ£€æŸ¥æ•°æ®é›†åˆ’åˆ†

**é¦–æ¬¡è®­ç»ƒå‰å¿…é¡»æ‰§è¡Œ**ï¼Œç¡®ä¿è®­ç»ƒé›†åŒ…å«æ‰€æœ‰ç±»åˆ«ï¼š

```bash
# 1. æ£€æŸ¥å½“å‰æ•°æ®é›†åˆ†å¸ƒ
python scripts/check_dataset_distribution.py

# å¦‚æœè®­ç»ƒé›†åªæœ‰å•ä¸€ç±»åˆ«ï¼ˆä¾‹å¦‚åªæœ‰ Goodï¼‰ï¼Œåˆ™éœ€è¦é‡æ–°åˆ’åˆ†ï¼š
# 2. é‡æ–°åˆ’åˆ†æ•°æ®é›†ï¼ˆ80/20 è®­ç»ƒ/æµ‹è¯•æ¯”ä¾‹ï¼‰
python scripts/resplit_dataset.py

# 3. éªŒè¯åˆ’åˆ†ç»“æœ
python scripts/check_dataset_distribution.py
```

**é¢„æœŸç»“æœ**:
```
è®­ç»ƒé›† (TRAIN):
  æ€»æ ·æœ¬æ•°: 3231
  ç±»åˆ«æ•°: 12  âœ… åŒ…å«æ‰€æœ‰ç±»åˆ«
  
æµ‹è¯•é›† (TEST):
  æ€»æ ·æœ¬æ•°: 809
  ç±»åˆ«æ•°: 12
```

å¦‚æœè®­ç»ƒé›†åªæœ‰ 1 ä¸ªç±»åˆ«ï¼ŒSupConLoss å°†æ— æ³•å·¥ä½œï¼ˆloss æ’å®šï¼‰ã€‚

### 5.1 é…ç½®è®­ç»ƒå‚æ•°

ç¼–è¾‘ `configs/train_config.py`:

```python
TRAIN_CONFIG = {
    # ä¼˜åŒ–å‚æ•°
    "batch_size": 16,          # æ ¹æ®GPUæ˜¾å­˜è°ƒæ•´ (16/32/64)
    "num_epochs": 100,
    "learning_rate": 1e-4,
    "weight_decay": 1e-4,
    "optimizer": "adamw",
    
    # å­¦ä¹ ç‡è°ƒåº¦
    "lr_scheduler": "cosine",
    "warmup_epochs": 5,
    "min_lr": 1e-6,
    
    # æŸå¤±å‡½æ•°
    "loss_type": "supcon",
    "temperature": 0.07,
    
    # è®­ç»ƒç­–ç•¥
    "gradient_clip": 1.0,
    "mixed_precision": True,   # å¼€å¯æ··åˆç²¾åº¦è®­ç»ƒ
    
    # è®¾å¤‡
    "device": "cuda",          # ä½¿ç”¨GPU
    "num_workers": 8,          # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
    
    # æ—¥å¿—
    "log_interval": 10,        # æ¯10ä¸ªbatchè®°å½•ä¸€æ¬¡
    "val_interval": 1,         # æ¯ä¸ªepochéªŒè¯ä¸€æ¬¡
    "save_interval": 5,        # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡
}
```

### 5.2 å¼€å§‹è®­ç»ƒ

**åŸºç¡€è®­ç»ƒ (ä½¿ç”¨é»˜è®¤é…ç½®):**
```bash
# ä½¿ç”¨è„šæœ¬å¯åŠ¨ï¼ˆè„šæœ¬å†…ä¼šè°ƒç”¨ `python src/train.py`ï¼‰
bash scripts/train.sh
```

**è‡ªå®šä¹‰è®­ç»ƒ (é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®):**
```bash
# ç›´æ¥è¿è¡Œï¼ˆé»˜è®¤ä½¿ç”¨ configs/train_config.py ä¸­çš„ deviceï¼Œä¼˜å…ˆä½¿ç”¨ CUDAï¼‰
python src/train.py

# å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆæŸ¥çœ‹æ‰¹æ¬¡æ ‡ç­¾åˆ†å¸ƒã€ç‰¹å¾ç»Ÿè®¡ç­‰ï¼‰
python src/train.py --debug

# æŒ‡å®šå‚æ•°ç¤ºä¾‹ï¼š
python src/train.py --batch-size 8 --device cuda --mixed-precision
python src/train.py --debug

# å¿«é€Ÿ smoke æµ‹è¯•ï¼ˆä½¿ç”¨çŸ­æœŸå°æ‰¹é‡å¹¶å¯ç”¨ dummyï¼Œä¾¿äºå¿«é€ŸéªŒè¯ï¼‰
python src/train.py --quick-test

# ä½¿ç”¨ dummy ç¼–ç å™¨å¹¶ä»…è·‘ 1 ä¸ª epochï¼ˆç¦»çº¿/æ— é¢„è®­ç»ƒæ¨¡å‹æ—¶æœ‰ç”¨ï¼‰
python src/train.py --use-dummy --num-epochs 1

# æˆ–ä½¿ç”¨ nohup åå°è¿è¡Œï¼ˆLinuxï¼‰
nohup bash scripts/train.sh > training.log 2>&1 &

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f training.log

# ç»˜åˆ¶è®­ç»ƒ/éªŒè¯æŸå¤±ä¸ç²¾åº¦æ›²çº¿
è¿è¡Œç»˜å›¾è„šæœ¬å°†ç”Ÿæˆåˆæˆå›¾ï¼ˆloss + accuracyï¼‰ï¼Œé»˜è®¤è¾“å‡ºåˆ° `outputs/loss_and_accuracy.png`ã€‚

```bash
bash scripts/plot_loss.sh
```

è®­ç»ƒè„šæœ¬ç°åœ¨åœ¨æ¯ä¸ª epoch çš„éªŒè¯é˜¶æ®µè®°å½•å¹¶æ‰“å°éªŒè¯å‡†ç¡®ç‡ï¼ˆ`val acc`ï¼‰ï¼Œè¯¥å€¼ä¼šå†™å…¥ `outputs/logs/training_log.json` çš„ `train` / `val` æ¡ç›®ä¸­ï¼Œç»˜å›¾è„šæœ¬ä¼šä¸€å¹¶ç»˜åˆ¶è®­ç»ƒ/éªŒè¯çš„ accuracy æ›²çº¿ã€‚

### ç»˜åˆ¶æ··æ·†çŸ©é˜µ (Confusion Matrix)

è®­ç»ƒå®Œæˆåï¼Œå¯ç”¨æœ€è¿‘ä¿å­˜çš„æœ€ä½³æ£€æŸ¥ç‚¹ç”ŸæˆéªŒè¯é›†ä¸Šçš„æ··æ·†çŸ©é˜µä»¥æŸ¥æ‰¾éš¾ç±»æˆ–ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚

é»˜è®¤ç›´æ¥è¿è¡Œï¼ˆåœ¨æœåŠ¡å™¨ä¸Šä¼šä¼˜å…ˆå°è¯•åŠ è½½ `/root/autodl-tmp/outputs/checkpoints/best_model.pth`ï¼Œè‹¥ä¸å­˜åœ¨è„šæœ¬ä¼šé€€å›åˆ° dummy æ¨¡å¼ç”Ÿæˆç¤ºä¾‹æ··æ·†çŸ©é˜µï¼‰ï¼š

```bat
python scripts/plot_confusion_matrix.py
```

å¯é€‰å‚æ•°ï¼š
- `--checkpoint PATH`ï¼šæŒ‡å®šæ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆé»˜è®¤ `/root/autodl-tmp/outputs/checkpoints/best_model.pth`ï¼‰ã€‚
- `--output PATH`ï¼šæŒ‡å®šä¿å­˜å›¾åƒè·¯å¾„ï¼ˆé»˜è®¤ `outputs/confusion_matrix.png`ï¼‰ã€‚
- `--metric {cosine,euclidean}`ï¼šé€‰æ‹©æœ€è¿‘è´¨å¿ƒé¢„æµ‹åº¦é‡ï¼ˆé»˜è®¤ cosineï¼‰ã€‚

**è¯Šæ–­è®­ç»ƒé—®é¢˜:**

å¦‚æœè®­ç»ƒ loss ä¸ä¸‹é™ï¼Œè¿è¡Œä»¥ä¸‹è¯Šæ–­è„šæœ¬ï¼š

```bash
# 1. æ£€æŸ¥é‡‡æ ·å™¨æ˜¯å¦æ­£ç¡®æ··åˆç±»åˆ«
python scripts/check_sampler.py

# 2. ä½¿ç”¨è°ƒè¯•æ¨¡å¼è®­ç»ƒï¼ˆæŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ï¼‰
python src/train.py --debug

# é¢„æœŸåœ¨è°ƒè¯•æ¨¡å¼ä¸‹çœ‹åˆ°ï¼š
# [DEBUG] Batch 0 labels - unique: [0, 1, 2, ...], counts: [3, 2, 4, ...]
# [DEBUG] âœ… Good! Batch contains X different classes
```

æ³¨æ„ï¼šé»˜è®¤è¾“å‡ºç›®å½•å·²é…ç½®ä¸º `/root/autodl-tmp/outputs`ï¼ˆåœ¨ `configs/train_config.py` ä¸­è®¾ç½®ï¼‰ã€‚

### 5.3 è®­ç»ƒè¾“å‡º

**å®æ—¶æ—¥å¿—:**
```
======================================================================
INITIALIZING MODEL
======================================================================
  Total parameters: 45,678,901
  Trainable parameters: 45,678,901
  Output dimension: 512
  Device: cuda

======================================================================
INITIALIZING DATA LOADERS
======================================================================
  Train samples: 800
  Val samples: 200
  Batch size: 16
  Train batches: 50
  Val batches: 13

======================================================================
INITIALIZING OPTIMIZER
======================================================================
  Optimizer: adamw
  Learning rate: 0.0001
  Weight decay: 0.0001
  Scheduler: cosine

======================================================================
INITIALIZING LOSS
======================================================================
  Loss: Supervised Contrastive
  Temperature: 0.07

======================================================================
STARTING TRAINING
======================================================================
  Epochs: 100
  Start time: 2025-10-10 14:23:45

Epoch 1/100
----------------------------------------------------------------------
  [  1][  1/ 50] Loss: 2.3456 | Avg: 2.3456 | LR: 1.00e-04
  [  1][ 10/ 50] Loss: 2.1234 | Avg: 2.2145 | LR: 1.00e-04
  [  1][ 20/ 50] Loss: 1.9876 | Avg: 2.1123 | LR: 1.00e-04
  [  1][ 30/ 50] Loss: 1.8543 | Avg: 2.0456 | LR: 1.00e-04
  [  1][ 40/ 50] Loss: 1.7654 | Avg: 1.9876 | LR: 1.00e-04
  [  1][ 50/ 50] Loss: 1.6789 | Avg: 1.9234 | LR: 1.00e-04
  Validation Loss: 2.0543
  âœ… Saved best model (epoch 1)
  Epoch time: 45.3s

Epoch 2/100
----------------------------------------------------------------------
  [  2][  1/ 50] Loss: 1.8765 | Avg: 1.8765 | LR: 9.50e-05
  ...
```

**ç»“æœä¿å­˜ä½ç½®:**
```
outputs/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ latest_model.pth         # æœ€æ–°æ¨¡å‹
â”‚   â”œâ”€â”€ best_model.pth           # æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±æœ€ä½)
â”‚   â”œâ”€â”€ epoch_005.pth      # ç¬¬5ä¸ªepoch
â”‚   â”œâ”€â”€ epoch_010.pth      # ç¬¬10ä¸ªepoch
â”‚   â””â”€â”€ ...
â””â”€â”€ logs/
    â””â”€â”€ training_log.json  # è®­ç»ƒå†å² (JSONæ ¼å¼)
```

### 5.4 ç›‘æ§è®­ç»ƒ

**æŸ¥çœ‹è®­ç»ƒæ—¥å¿—:**
```bash
# æŸ¥çœ‹ JSON æ—¥å¿—
python -m json.tool /root/autodl-tmp/outputs/logs/training_log.json

# æå–å…³é”®æŒ‡æ ‡
python -c "
import json
with open('/root/autodl-tmp/outputs/logs/training_log.json') as f:
    log = json.load(f)
    train_losses = [m['loss'] for m in log['train']]
    val_losses = [m['loss'] for m in log['val']]
    print(f'Train loss: {train_losses[-1]:.4f}')
    print(f'Val loss: {val_losses[-1]:.4f}')
    print(f'Best val loss: {log[\"best_metric\"]:.4f}')
"
```

**å¯é€‰: ä½¿ç”¨ TensorBoard å¯è§†åŒ–**
```bash
# å®‰è£… TensorBoard
pip install tensorboard

# å¯åŠ¨ (å¦‚æœå®ç°äº† TensorBoard é›†æˆ)
tensorboard --port 6007 --logdir /root/autodl-tmp/outputs/logs
```

---

## 6. é¢„æœŸè¾“å‡ºæ±‡æ€»

### 6.1 æµ‹è¯•è¾“å‡º

| æµ‹è¯•æ¨¡å— | æµ‹è¯•è„šæœ¬ | é¢„æœŸè€—æ—¶ | è¾“å‡ºç»“æœ | ä¿å­˜ä½ç½® |
|---------|---------|---------|---------|---------|
| æ•°æ®åŠ è½½ | `test_dataset.sh` | ~10s | âœ… æ‰€æœ‰æ•°æ®å½¢çŠ¶æ­£ç¡® | æ—  |
| ç¼–ç å™¨ | `test_encoders.sh` | ~20s | âœ… 4ä¸ªç¼–ç å™¨å…¨éƒ¨é€šè¿‡ | æ—  |
| èåˆæ¨¡å— | `test_fusion.sh` | ~15s | âœ… èåˆå’Œæ³¨æ„åŠ›æ­£ç¡® | æ—  |
| å®Œæ•´æ¨¡å‹ | `test_model.sh` | ~30s | âœ… ç«¯åˆ°ç«¯æµ‹è¯•é€šè¿‡ | æ—  |
| æŸå¤±å‡½æ•° | `test_losses.sh` | ~5s | âœ… SupConLoss æ­£ç¡® | æ—  |

### 6.2 è®­ç»ƒè¾“å‡º

| è¾“å‡ºç±»å‹ | æ–‡ä»¶è·¯å¾„ | å†…å®¹ |
|---------|---------|------|
| æœ€æ–°æ¨¡å‹ | `/root/autodl-tmp/outputs/checkpoints/latest_model.pth` | æœ€åä¸€ä¸ªepochçš„æ¨¡å‹çŠ¶æ€ |
| æœ€ä½³æ¨¡å‹ | `/root/autodl-tmp/outputs/checkpoints/best_model.pth` | éªŒè¯æŸå¤±æœ€ä½çš„æ¨¡å‹ |
| å‘¨æœŸæ£€æŸ¥ç‚¹ | `/root/autodl-tmp/outputs/checkpoints/epoch_XXX.pth` | å®šæœŸä¿å­˜çš„æ¨¡å‹ |
| è®­ç»ƒæ—¥å¿— | `/root/autodl-tmp/outputs/logs/training_log.json` | å®Œæ•´è®­ç»ƒå†å²(æŸå¤±ã€å­¦ä¹ ç‡ç­‰) |

**è®­ç»ƒæ—¥å¿—ç»“æ„ (training_log.json):**
```json
{
  "train": [
    {"train/loss": 2.3456, "train/time": 45.3, "train/lr": 0.0001},
    {"train/loss": 1.8765, "train/time": 44.8, "train/lr": 0.000095},
    ...
  ],
  "val": [
    {"val/loss": 2.0543},
    {"val/loss": 1.9234},
    ...
  ],
  "config": {
    "batch_size": 16,
    "num_epochs": 100,
    ...
  },
  "best_metric": 1.2345
}
```

---
bash scripts/evaluate.sh

### è¯„ä¼°ç»“æœæ€»ç»“åˆ†æ

**å®éªŒç»“æœ**ï¼šå››æ¨¡æ€æ·±åº¦èåˆæ¨¡å‹åœ¨æµ‹è¯•é›†ï¼ˆ809æ ·æœ¬ï¼‰ä¸Šè¾¾åˆ°99.13%å‡†ç¡®ç‡ï¼ŒF1åˆ†æ•°99.14%ï¼Œæ··æ·†çŸ©é˜µæ˜¾ç¤ºä»…å°‘æ•°è¯¯åˆ†ç±»ã€‚

**å…¬æ­£æ€§éªŒè¯**ï¼š
- âœ… åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼Œæ— æ•°æ®æ³„éœ²
- âœ… æ ·æœ¬åˆ’åˆ†å›ºå®šï¼ˆTRAIN:3231, TEST:809ï¼‰ï¼Œæ¯æ¬¡è¿è¡Œä¸€è‡´
- âœ… ä½¿ç”¨k-NNåˆ†ç±»å™¨ï¼ˆk=5ï¼Œä½™å¼¦è·ç¦»ï¼‰è¯„ä¼°ç‰¹å¾è´¨é‡

**åˆç†æ€§åˆ†æ**ï¼š
- ç»“æœæ˜¾è‘—è¶…è¶Šå…¸å‹ç„Šæ¥ç¼ºé™·æ£€æµ‹è®ºæ–‡ï¼ˆé€šå¸¸80-95%å‡†ç¡®ç‡ï¼‰
- è¯æ˜å››æ¨¡æ€æ·±åº¦èåˆç­–ç•¥æœ‰æ•ˆï¼Œå®ç°äº†READMEä¸­â€œè¶…è¶Šæ‰€æœ‰åŸºçº¿â€çš„ç›®æ ‡
- ç±»åˆ«å¹³è¡¡è‰¯å¥½ï¼Œå°‘æ•°ç±»ï¼ˆå¦‚ç±»åˆ«1ã€2ï¼‰å¬å›ç‡>97%

**ç»“è®º**ï¼šæ¨¡å‹æ€§èƒ½ä¼˜ç§€ï¼ŒæŠ€æœ¯æ–¹æ¡ˆéªŒè¯æˆåŠŸï¼Œå¯ç”¨äºå®é™…ç„Šæ¥è´¨é‡æ£€æµ‹ã€‚

## 7. æ•…éšœæ’æŸ¥

### 7.1 å¸¸è§é—®é¢˜

**é—®é¢˜ 1: ImportError: No module named 'torch'**
```bash
# è§£å†³æ–¹æ¡ˆ: é‡æ–°å®‰è£… PyTorch
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**é—®é¢˜ 2: CUDA out of memory**
```bash
# è§£å†³æ–¹æ¡ˆ: å‡å° batch_size
# ç¼–è¾‘ configs/train_config.py:
"batch_size": 8,  # ä»16æ”¹ä¸º8
```

**é—®é¢˜ 3: æ¨¡å‹åŠ è½½å¤±è´¥ (transformers)**
```bash
# è§£å†³æ–¹æ¡ˆ: ä½¿ç”¨ Dummy æ¨¡å¼æµ‹è¯•
# æµ‹è¯•è„šæœ¬å·²ç»é»˜è®¤ä½¿ç”¨ dummy=True
# æˆ–è€…æ£€æŸ¥ç½‘ç»œè¿æ¥ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
```

**é—®é¢˜ 4: DataLoader num_workers é”™è¯¯ (Windows)**
```bash
# è§£å†³æ–¹æ¡ˆ: å‡å°‘ worker æ•°é‡
# ç¼–è¾‘ configs/train_config.py:
"num_workers": 0,  # Windows ä¸‹ä½¿ç”¨ 0
```

### 7.2 éªŒè¯å®‰è£…

**å¿«é€ŸéªŒè¯è„šæœ¬:**
```bash
python << EOF
import sys
import torch
print(f"Python: {sys.version}")
print(f"PyTorch: {torch.__version__}")
print(f"CUDA: {torch.cuda.is_available()}")

# æµ‹è¯•å¯¼å…¥
sys.path.insert(0, '.')
from src.models import QuadModalSOTAModel
from src.losses import SupConLoss
from src.dataset import WeldingDataset

print("âœ… All imports successful!")
EOF
```

### 7.3 è·å–å¸®åŠ©

**æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£:**
- æŠ€æœ¯æ–¹æ¡ˆ: `README.md`
- å®ç°è¿›åº¦: `docs/PROGRESS_QUADMODAL.md`
- é¡¹ç›®ç»“æ„: `docs/PROJECT_STRUCTURE.md`

**æ£€æŸ¥ä»£ç :**
- æ¨¡å‹å®šä¹‰: `src/models/`
- é…ç½®æ–‡ä»¶: `configs/`
- æµ‹è¯•è„šæœ¬: `tests/`

---

## ğŸ¯ ä¸‹ä¸€æ­¥

å®Œæˆä¸Šè¿°æ­¥éª¤å,æ‚¨åº”è¯¥èƒ½å¤Ÿ:
- âœ… æˆåŠŸè¿è¡Œæ‰€æœ‰æµ‹è¯•
- âœ… ç†è§£å››æ¨¡æ€æ¶æ„
- âœ… å¼€å§‹è®­ç»ƒè‡ªå·±çš„æ¨¡å‹
- âœ… åœ¨æœåŠ¡å™¨ä¸Šéƒ¨ç½²è®­ç»ƒ

**æ¨èå­¦ä¹ è·¯å¾„:**
1. ç†è§£æ¯ä¸ªç¼–ç å™¨çš„å·¥ä½œåŸç† (`src/models/`)
2. ç ”ç©¶äº¤å‰æ³¨æ„åŠ›èåˆæœºåˆ¶ (`src/models/fusion.py`)
3. è°ƒæ•´è®­ç»ƒé…ç½®ä¼˜åŒ–æ€§èƒ½ (`configs/train_config.py`)
4. å®éªŒä¸åŒçš„è¶…å‚æ•°ç»„åˆ

**è¿›é˜¶ä»»åŠ¡:**
- å®ç° k-NN è¯„ä¼°åè®® (Step 6)
- æ·»åŠ æ›´å¤šæ•°æ®å¢å¼ºç­–ç•¥
- å®éªŒä¸åŒçš„èåˆç­–ç•¥
- å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡

---

**å¿«é€Ÿå¼€å§‹æŒ‡å—æ›´æ–°å®Œæˆ!** ğŸ“š
